# Data and results folder

Dataset in final format is located [here](https://github.com/eth-sri/SynthPAI/blob/main/data/synthpai.jsonl).
You can access the final dataset with evaluation results from the original paper [here](https://github.com/eth-sri/SynthPAI/blob/main/data/synthpai_merged_evals.jsonl).

## Synthetic profiles dataset

- [`/data/profiles`](https://github.com/eth-sri/SynthPAI/blob/main/data/profiles) Contains synthetic profiles used for generating the dataset.
  - [`/data/profiles/user_bot_profiles_300.json`](https://github.com/eth-sri/SynthPAI/blob/main/data/profiles/user_bot_profiles_300.json) Contains 300 synthetic profiles.
  - [`/data/profiles/user_bot_gen_online_profiles_300.json`](https://github.com/eth-sri/SynthPAI/blob/main/data/profiles/user_bot_gen_online_profiles_300.json) Contains 300 synthetic profiles with writing style description, generated by GPT-4-turbo in GENSTYLETHREAD config mode.

If the user wishes to create more synthetic profiles, then it needs to be done manually by using the prompt from the original paper as an option. It is recommended to update the [original file](https://github.com/eth-sri/SynthPAI/blob/main/data/profiles/user_bot_profiles_300.json) with new profile dictionaries.

Aftewards online writing styles need to be generated for new synthetic profiles. For this, you can use the following command 

```bash

python main.py --config_file configs\thread\generate_styles.yaml

```

This script will regenerate the styles and update [the dataset with online writing styles](https://github.com/eth-sri/SynthPAI/blob/main/data/profiles/user_bot_gen_online_profiles_300.json).

## Generated threads

Generated threads are located in [`/data/thread/generated_threads`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/generated_threads) folder in three formats available - JSON, HTML and TXT. Folder [`/data/thread/gens_per_profile`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/gens_per_profile) contains generated comments on level of every synthetic profile.

Comments from generated threads were collected into combined files, like, for example, [`/data/thread/comments_unique_id.jsonl`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/comments_unique_id.jsonl). If the user wants to access every model tag/label separately, there is file [`/data/thread/comments.jsonl`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/comments.jsonl), where comments might be duplicated if they have labels for more than 1 feature.

After human manual check the comments with human labels are saved in [`/data/thread/synth_clean.jsonl`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/synth_clean.jsonl). After running scripts there will be generated a file which should be used for inference evaluation - [`/data/thread/comments_eval.jsonl`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/comments_eval.jsonl). The comments are being aggregated on profile level by collecting all comments in one list and taking the minimal hardness/certainty level label for every feature if present. Those labels can later be manually filtered out (dropped out if accuracy is 0) and saved in [`/data/thread/comments_eval_revised.jsonl`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/comments_eval_revised.jsonl) file. This file is recommended to use for evaluation pipeline.

## LLM inference evaluation

Two folders contain results for prediction and evaluation output for LLM inference process:

- [`/data/thread/predicted`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/predicted) Contains LLM inference results for every model used.
- [`/data/thread/eval`](https://github.com/eth-sri/SynthPAI/blob/main/data/thread/eval) Contains inference evaluation results for every model used.

Predictions are stored in `[model_name]_predicted.jsonl` file in value dict for `predictions` key for every profile. Files `*_fixed.jsonl` means that the model output was normalized if it did not follow the instructed output format closely and the list of answers was extracted by running normalization script.
Evaluation results are stored in `[model_name]_evaluated.jsonl` file in value dict for `evaluations` key.

To keep our paper experiments transparent, we have saved evaluation runs in `decider=model` mode in `[model_name]_revised_human_evaluated.jsonl` (against human labels) and `[model_name]_gt_evaluated.jsonl` (against ground truth labels) files. Howevre, the evaluation logic of LLM judging model is not consistent and is not reliable with feature value real meaning. Therefore, the authors recommend using `decider=model_human` mode to double-check LLM's evaluation. All the files named in format `[model_name]_evaluated.jsonl` were done in this mode.